from __future__ import print_function
from __future__ import division
from . import C_

import torch
from torch.autograd import Variable
from ..myUtils import prints

###################################################################################################################################################

def get_mount_gpu(data, gpu):
	if isinstance(data, dict):
		data = {key:Variable(data[key].to(gpu), requires_grad=False) for key in data.keys()}
	else:
		data = Variable(data.to(gpu), requires_grad=False)
	return data

def get_data_target_gpu(data, target, gpu):
	data = get_mount_gpu(data, gpu)
	target = get_mount_gpu(target, gpu)
	return data, target

def tensor_to_numpy(x):
	return x.cpu().detach().numpy()

def print_gpu_info(): # PyTorch v0.4.0
	prints.print_green(f'GPUs: {torch.cuda.device_count()}') # PyTorch v0.4.0

def print_gpu_name(gpu_index=0):
	prints.print_green(f'GPU index: {gpu_index} name: {torch.cuda.get_device_name(gpu_index)}')

def print_gpu_names():
	gpus = torch.cuda.device_count()
	if gpus==0:
		prints.print_green('there are no GPUs >:c')
	else:
		for i in range(gpus):
			print_gpu_name(i)