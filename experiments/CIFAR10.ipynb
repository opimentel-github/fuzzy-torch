{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../flaming-choripan') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "## Train-Val Split\n",
    "train_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':True,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "val_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':False,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "train_cifar10 = datasets.CIFAR10(**train_kwargs)\n",
    "val_cifar10 = datasets.CIFAR10(**val_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([50000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([50000]) - y.max: 9\n",
      "x: torch.Size([10000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([10000]) - y.max: 9\n",
      "{'input': {'x': (3, 32, 32)-float32-cpu, 'x2': (32)-float32-cpu}, 'target': {'y': ()-int64-cpu, 'y2': (1)-int64-cpu}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "## Batch Sizes\n",
    "train_batch_size = 256\n",
    "val_batch_size = train_batch_size\n",
    "\n",
    "train_dataset_mnist = MyDataset(train_cifar10.data, train_cifar10.targets, uses_da=True)\n",
    "val_dataset_mnist = MyDataset(val_cifar10.data, val_cifar10.targets)\n",
    "val_dataset_mnist.set_norm_values(*train_dataset_mnist.get_norm_values())\n",
    "\n",
    "print(train_dataset_mnist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input': {'x': (256, 3, 32, 32)-float32-cpu, 'x2': (256, 32)-float32-cpu}, 'target': {'y': (256)-int64-cpu, 'y2': (256, 1)-int64-cpu}}\n",
      "data torch.Size([256, 3, 32, 32]) cpu torch.float32 tensor(-2.1466) tensor(2.3136)\n",
      "target torch.Size([256]) cpu torch.int64 tensor(0) tensor(9)\n",
      "196\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAATWUlEQVR4nO3dfbBV1XnH8e9PvPImighBRCJqNUoswZSojUxGE+OozcRk2qbaJqWtHTJtbc1MM6mTvoRkMq12akz/aMyQarVOa5rEGI1jkxDHxpoXAxhEhKoYUSQIKiiIChd5+sfeTC/0rHUP55V71+8zc+aes9bZez93z33u2mevs9ZSRGBmo99h/Q7AzHrDyW5WCCe7WSGc7GaFcLKbFcLJblYIJ7tZIZzsI5Sk9ZIu7OPxb5H0+YPc5n5JL0jaLukRSZd1Kz77/w7vdwDWH5LGRMSbPT7s1cCaiNgj6Rzg+5JOi4hNPY6jSG7ZRyBJtwFvBb4t6VVJn6rLvy7peUmvSHpA0tuHbHOLpBsl3StpJ3CBpGMlfbtuaZdJ+rykB4dsc7qkpZK2Snpc0kfq8kXA7wCfqo//7WbijohVEbFn30tgAJjViXNiTYgIP0bgA1gPXHhA2R8Ak4CxwBeBlUPqbgFeAc6j+ic/Dvhq/ZgAzAE2AA/W759Yv/59qivAs4AXgTlD9vf5A47/JeBLw8R9D/AGVbJ/Bzis3+eylIcv40eRiLh533NJi4Ftko6OiFfq4rsi4od1/SDw68CZEfEasEbSrcD59Xs/AKyPiH+pX/9M0h3AbwKfTRz/j5uI8QOSBoALgTMiYu9B/prWIl/GjxKSxki6VtJTkrZTtfwAU4e8bcOQ59OoWuwNifoTgXMkvbzvQXXpfly7sUbEYET8J3CRpA+2uz9rjlv2kevA4Yq/DVxG1WKuB44GtgFKbPMCsAc4AXiiLhv6+XkD8IOIeH+Tx2/F4cApHdiPNcEt+8i1GTh5yOtJwC7gJarP4H+b2ziqO/HfBBZLmiDpdOB3h7zlHuA0SR+TNFA/3iXpjMTxs+qbfZdIGl/v66PAe4AfNLsPa4+TfeT6O+Cv6kvsTwL/CjwDbATWAD9pYh9XUV0BPA/cBtxO9Q+DiNgBXARcDvyifs91VDf/AG4C5tTH/xaApC9L+nLiWAIWA1uoriquBn4rIh4+iN/Z2qD6DqkZkq4DjouIhf2OxTrPLXvB6kvruaqcDVwJ3NnvuKw7fIOubJOoLt2Pp/oMfj1wV18jsq7xZbxZIXwZb1aInl7GS/JlRCGmT2xcfsSY9DYbtncnloZxZOoSoQNwWKZ5jMxOM782ExLfIdw1kN5mfGKHL7wOO3aHGtW1leySLgb+kep3+eeIuLad/dnIkrssXPjLjctnTU5v86ffaSucgzIzU/euTN3E8em6wdnpusmZZm7u643Lnz4+vc3bj25c/tcPNi6HNi7jJY0B/gm4hGoQxRWS5rS6PzPrrnY+s58NrIuIn0fEbqrRU56MwOwQ1U6yz2T/gRPP0eDqSNIiScslLW/jWGbWpq7foIuIJcAS8A06s35qp2XfyP6jpE6oy8zsENROy74MOFXSSVRJfjnVMEsrRG7Wicc3Ny6fdFpXQjloH7wgXffFb3wpXTmQ6Q+blLitDvDKMem6nVMalx+fmSJwsHEcN5z7Z8lNWk72qCYNvAr4LlXX280R8Vir+zOz7mrrM3tE3Avc26FYzKyL/HVZs0I42c0K4WQ3K4ST3awQnrzCumLw6cblLyUGcABkxpiQ6dRqyZm5URxTpmUq92TqMr/B2Ew32tFjG5dv2JHeZtaRjcvVcMAb4JbdrBhOdrNCONnNCuFkNyuEk92sEL4bPwLl/kOnbnZv60YgGanp2B7clN4md5/7qEzda5m61D5fS4w9AWD3T9N1cWK67vXM3fjJuzIHfLZx8fTc5FmpO/Xpu/5u2c0K4WQ3K4ST3awQTnazQjjZzQrhZDcrhLveRqDc3G+zT2+8nsnknQ8nt3l6Q2aQRkZ6yAWM/8MFDcuPWvqT5DaDmc63XIS585FyxoRM5REnp+u2vpSuOzy3yNPOdNUvdjcu1wvpbWbMTVSk22+37GaFcLKbFcLJblYIJ7tZIZzsZoVwspsVwl1vo8z6J5Y1LF9wwfzkNqceNy5Zt3TZg8m6OdPTcZywofFYtKnzz01us+KZ9LFeSR+qJW+uznSTbR9M1x2VSZnXc52Ax2WqTmpcviuzwNKexBqpkV47ta1kl7Seaqzdm8CeiEj/RZlZX3WiZb8gIl7swH7MrIv8md2sEO0mewDfk7RC0qJGb5C0SNJyScvbPJaZtaHdy/gFEbFR0luApZL+JyIeGPqGiFgCLAGQlL57YGZd1VbLHhEb659bgDuBszsRlJl1Xsstu6SJwGERsaN+fhHwuY5FZi3Zluj9WXpf+lNUbtmlgczQtt2b03WP3Nd4lN2CK85IbpPrXhvI1GU6ypK2Tc9cZE7IdMu9mqmbPCNd99rWdF0kfvMJiWWhAJQ4+UqPHGznMn46cKeqtaUOB/49Ir7Txv7MrItaTvaI+Dnwjg7GYmZd5K43s0I42c0K4WQ3K4ST3awQHvVWiDdarCPTQ5XrKluVKJ+2bm3uaEm57rVjMnWpNe4e35wZoXZ45mjjM3W5vkgy3Wip42laZn/HJsrTIxjdspsVwsluVggnu1khnOxmhXCymxXCd+OtZVtylYnxGM8819qx3pqpy90DT5k9NVM5mOmfiNPTdWNzEzZllnLamjhZrx+d3mZmInUjvVCWW3azQjjZzQrhZDcrhJPdrBBOdrNCONnNCjGiu94mT0wPFHh5Z6arw/omt7LS5Mx2z7Z4vNSwkO2PZDZ6NTPEZ0yme219Jp3GZeaumzKxcfnuzOCZPYmBPJmBS27ZzQrhZDcrhJPdrBBOdrNCONnNCuFkNyvEiO56c/fayLN2d7pu7tx03YrUpHbDSI1fe/bEzEZb1mUqM2PsdmQW0nop07F4xKbG5WsySyee+VTj8j2vJzcZtmWXdLOkLZJWDymbImmppCfrn7k5/8zsENDMZfwtwMUHlF0D3BcRpwL31a/N7BA2bLLX660fuATlZcCt9fNbgQ91OC4z67BWP7NPj4h9HzSep1rRtSFJi4BFLR7HzDqk7Rt0ERGSkt/IjYglwBKA3PvMrLta7XrbLGkGQP0zOx2ZmfVfqy373cBC4Nr6510di8hGtcED7/4MkVt1qdPeWJ2uW73w68m6vZnBa7sS80YCvJhpDgemNC5/y/HpbcY/n4gh0SMHzXW93Q78GHibpOckXUmV5O+X9CRwYf3azA5hw7bsEXFFoup9HY7FzLrIX5c1K4ST3awQTnazQjjZzQoxoke92egy7cx02zPux4kJFkmPbIN0a7YtvTsGX07Xrd2Yrjsy+T1SmJIZ7bcjMcjujdfS2wxOalyeXunNLbtZMZzsZoVwspsVwsluVggnu1khnOxmhXDXmx0yxm5MT2X4a+9+KVn3rR+l95nqippxVHqbY96ernvHaem6yNTNyCxWd2xqQbqz0tuQWI5uwnPpTdyymxXCyW5WCCe7WSGc7GaFcLKbFcJ34+2Qccez6Tvu786M8Fj4K+m67z/duHzcCeltZsxL1w3en67LZdO4zJ16EnPQ8WJmm6mJ8swceW7ZzQrhZDcrhJPdrBBOdrNCONnNCuFkNyuEInq31qIXdrReO+e8xv1h50w+ObnNsRueSNadlxkk875Ml92GzECYqdMal49PDHYBYGLj4vk3wfJfhBrVNbP8082StkhaPaRssaSNklbWj0uH24+Z9Vczl/G3ABc3KL8hIubVj3s7G5aZddqwyR4RDwCZtTfNbCRo5wbdVZJW1Zf5yVkHJC2StFzS8jaOZWZtajXZbwROAeYBm4DrU2+MiCURMT8i5rd4LDPrgJaSPSI2R8SbEbEX+ApwdmfDMrNOa2nUm6QZEbGpfvlhYHXu/Wb98tAP9zQuJ929lrUqXfXpZem6vdvSdesmNC5/dUN6m9QgwNxvNWyyS7odOB+YKuk54DPA+ZLmAQGsBz4+3H7MrL+GTfaIuKJB8U1diMXMushflzUrhJPdrBBOdrNCONnNCuFRb9ZTmfkQk91JdnAiWhz1Zmajg5PdrBBOdrNCONnNCuFkNyuEk92sEF7rzXpqXKZuZ8+iKJNbdrNCONnNCuFkNyuEk92sEE52s0J4IIzZKOOBMGaFc7KbFcLJblYIJ7tZIZzsZoVwspsVYthklzRL0v2S1kh6TNLVdfkUSUslPVn/TK7kamb9N2w/u6QZwIyIeFjSJGAF8CHg94CtEXGtpGuAYyLiL4bZl/vZzbqs5X72iNgUEQ/Xz3cAa4GZwGXArfXbbqX6B2Bmh6iD+swuaTZwFvAQMH3ISq7PA9M7GpmZdVTTk1dIOhK4A/hERGyX/u9KISIidYkuaRGwqN1Azaw9TX03XtIAcA/w3Yj4Ql32OHB+RGyqP9f/V0S8bZj9+DO7WZe1/JldVRN+E7B2X6LX7gYW1s8XAne1G6SZdU8zd+MXAP8NPArsrYs/TfW5/WvAW4FngI9ExNZh9uWW3azLUi27h7iajTIe4mpWOCe7WSGc7GaFcLKbFcLJblYIL/9kPZVb/umNnkVRJrfsZoVwspsVwsluVggnu1khnOxmhXCymxXCXW/WU2Mzde566y637GaFcLKbFcLJblYIJ7tZIZzsZoXw3XjrqdMzdcsydXszddYct+xmhXCymxXCyW5WCCe7WSGc7GaFcLKbFaKZtd5mSbpf0hpJj0m6ui5fLGmjpJX149Luh2sj3YTMYyDzsPY108++B/jziHhY0iRghaSldd0NEfEP3QvPzDpl2GSPiE3Apvr5DklrgZndDszMOuugPrNLmg2cRbWCK8BVklZJulnSMR2Ozcw6qOlkl3QkcAfwiYjYDtwInALMo2r5r09st0jScknLOxCvmbWoqSWbJQ0A9wDfjYgvNKifDdwTEWcOsx8v2Vy4CzJ1P8rU7ep0IKNYy0s2SxJwE7B2aKJLmjHkbR8GVrcbpJl1TzN3488DPgY8KmllXfZp4ApJ84AA1gMf70qENqpsy9S59e6upi7jO3YwX8YXb16mbmWmzprX8mW8mY0OTnazQjjZzQrhZDcrhJPdrBCecNJ6Kte65Oo84WT73LKbFcLJblYIJ7tZIZzsZoVwspsVwsluVgh3vfXRrEzdhp5F0VsvZ+pyUx291OlACuSW3awQTnazQjjZzQrhZDcrhJPdrBBOdrNCuOutj0qckO+kTN2DPYuiTG7ZzQrhZDcrhJPdrBBOdrNCONnNCjHs3XhJ44AHgLH1+78REZ+RdBLwVeBYYAXwsYjY3c1gR5ud/Q6gD17J1I3N1HlpqPY107LvAt4bEe+gWr3nYknnAtcBN0TEL1Et4XVl98I0s3YNm+xRebV+OVA/Angv8I26/FbgQ12J0Mw6oqnP7JLG1Cu4bgGWAk8BL0fEnvotzwEzuxOimXVCU8keEW9GxDzgBOBs4PRmDyBpkaTlkpa3GKOZdcBB3Y2PiJeB+4FfBSZL2neD7wRgY2KbJRExPyLmtxWpmbVl2GSXNE3S5Pr5eOD9wFqqpP+N+m0Lgbu6FaSZtU8R+eEYkuZS3YAbQ/XP4WsR8TlJJ1N1vU0BfgZ8NCKyPSSSShz7kZS7ydHwMmkUmJqpOzpT91SnAxnFIkKNyodN9k5ysu/Pyb4/J3tnpJLd36AzK4ST3awQTnazQjjZzQrhZDcrRK/vxr8APFO/nAq82LODpzmO/TmO/Y20OE6MiGmNKnqa7PsdWFp+KHyrznE4jlLi8GW8WSGc7GaF6GeyL+njsYdyHPtzHPsbNXH07TO7mfWWL+PNCuFkNytEX5Jd0sWSHpe0TtI1/YihjmO9pEclrezlTDqSbpa0RdLqIWVTJC2V9GT985g+xbFY0sb6nKyUdGkP4pgl6X5JayQ9Junquryn5yQTR0/PiaRxkn4q6ZE6js/W5SdJeqjOm/+QdMRB7TgievqgGhf/FHAycATwCDCn13HUsawHpvbhuO8B3gmsHlL298A19fNrgOv6FMdi4JM9Ph8zgHfWzycBTwBzen1OMnH09JwAAo6snw8ADwHnAl8DLq/Lvwz80cHstx8t+9nAuoj4eVTzzH8VuKwPcfRNRDwAbD2g+DKqSUKgR7P1JuLouYjYFBEP1893UM2ENJMen5NMHD0VlY7P6NyPZJ8JbBjyup8z0wbwPUkrJC3qUwz7TI+ITfXz54HpfYzlKkmr6sv8rn+cGErSbOAsqtasb+fkgDigx+ekGzM6l36DbkFEvBO4BPgTSe/pd0BQ/Wenf8u33wicQrUgyCbg+l4dWNKRwB3AJyJi+9C6Xp6TBnH0/JxEGzM6p/Qj2TcCs4a8Ts5M220RsbH+uQW4k+qk9stmSTMA6p9b+hFERGyu/9D2Al+hR+dE0gBVgv1bRHyzLu75OWkUR7/OSX3sg57ROaUfyb4MOLW+s3gEcDlwd6+DkDRR0qR9z4GLgNX5rbrqbqpZeqGPs/XuS67ah+nBOZEk4CZgbUR8YUhVT89JKo5en5OuzejcqzuMB9xtvJTqTudTwF/2KYaTqXoCHgEe62UcwO1Ul4ODVJ+9rqRaIPM+4Eng+8CUPsVxG/AosIoq2Wb0II4FVJfoq4CV9ePSXp+TTBw9PSfAXKoZm1dR/WP5myF/sz8F1gFfB8YezH79dVmzQpR+g86sGE52s0I42c0K4WQ3K4ST3awQTnazQjjZzQrxv1nIXrh632LoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzytorch.datasets import tensor_data_collate\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist, batch_size=train_batch_size, shuffle=True, collate_fn=tensor_data_collate)\n",
    "val_loader_mnist = torch.utils.data.DataLoader(val_dataset_mnist, batch_size=val_batch_size, collate_fn=tensor_data_collate)\n",
    "\n",
    "# print example\n",
    "for k,tensor_dict in enumerate(train_loader_mnist):\n",
    "#for k,(data, target) in enumerate(val_loader_mnist):\n",
    "    print(tensor_dict)\n",
    "    ind = 37\n",
    "    data = tensor_dict['input']['x']\n",
    "    target = tensor_dict['target']['y']\n",
    "    print('data', data.shape, data.device ,data.dtype, data.min(), data.max())\n",
    "    print('target', target.shape, target.device, target.dtype, target.min(), target.max())\n",
    "    img = data[ind].permute(1,2,0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'target: {target[ind]}')\n",
    "    break\n",
    "    \n",
    "print(len(train_loader_mnist))\n",
    "print(len(val_loader_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(0) - {'mdl_class': <class 'baseline_models.CNN2DClassifier'>, 'mdl_kwargs': {'dropout': 0.5, 'cnn_features': [16, 32, 64], 'uses_mlp_classifier': True}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flamingchoripan.datascience.grid_search import GDIter, GridSeacher\n",
    "from baseline_models import MLPClassifier, CNN2DClassifier\n",
    "\n",
    "mdl_params = {\n",
    "    #'mdl_class':MLPClassifier,\n",
    "    'mdl_class':CNN2DClassifier,\n",
    "    'mdl_kwargs':{\n",
    "        'dropout':0.5,\n",
    "        #'dropout':0.0,\n",
    "        'cnn_features':[16, 32, 64],\n",
    "        #'cnn_features':[16, 32],\n",
    "        'uses_mlp_classifier':True,\n",
    "        #'uses_mlp_classifier':False,\n",
    "    },\n",
    "}\n",
    "gs = GridSeacher(mdl_params)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ml_cnn2d: Conv2D(\n",
      "  (0) - Conv2DLinear(input_dims=3, input_space=[32, 32], output_dims=16, output_space=[16, 16], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(1,216[p])\n",
      "  (1) - Conv2DLinear(input_dims=16, input_space=[16, 16], output_dims=32, output_space=[8, 8], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(12,832[p])\n",
      "  (2) - Conv2DLinear(input_dims=32, input_space=[8, 8], output_dims=64, output_space=[4, 4], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(51,264[p])\n",
      ")(65,312[p])\n",
      "mlp_classifier: MLP(\n",
      "  (0) - Linear(input_dims=1024, output_dims=50, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(51,250[p])\n",
      "  (1) - Linear(input_dims=50, output_dims=10, activation=linear, in_dropout=0.5, out_dropout=0.0, bias=True, split_out=1)(510[p])\n",
      ")(51,760[p])\n",
      "▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34mmodel_name: mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64 - id: 0\u001b[0m\n",
      "\u001b[32mdevice: cpu - device_name: cpu\u001b[0m\n",
      "save_rootdir: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64\n",
      "[x-entropy]\n",
      " - opt-parameters: 117,072[p] - device: cpu\n",
      " - save-mode: only_sup_metric(target_metric_crit: accuracy)\n",
      " - counter_k: k: 0/0 - counter_epoch: val_epoch: 0/0 > earlystop_epoch: 0/21\n",
      "\u001b[31m> deleting previous epochs [2, 1] in: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64 (id: 0)\u001b[0m\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  0%|          | 11/196000 [02:57, 16.06s/it, id: 0 - epoch: 12/1,000(6/196)[x-entropy] __loss__: 1.77=.89+.59(loss/2+loss/3) @.088[segs]\u001b[34m[train][x-entropy] __loss__: 1.69=.85+.56(loss/2+loss/3) - accuracy: 39.31 - dummy-accuracy: 10.00 @13.138[segs]\u001b[0m\u001b[31m[val][x-entropy] __loss__: 1.69=.84+.56(loss/2+loss/3) - accuracy: 39.66 - dummy-accuracy: 10.00 @1.934[segs]\u001b[0m\u001b[33m[stop][x-entropy] counter_epoch: val_epoch: 0/0 > earlystop_epoch: 1/21)\u001b[0m] "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### LOSS\n",
    "from fuzzytorch.losses import CrossEntropy\n",
    "\n",
    "loss_kwargs = {\n",
    "    'model_output_is_with_softmax':False,\n",
    "    'target_is_onehot':False,\n",
    "}\n",
    "loss = CrossEntropy('x-entropy', **loss_kwargs)\n",
    "\n",
    "### METRICS\n",
    "from fuzzytorch.metrics import DummyAccuracy, OnehotAccuracy\n",
    "metrics = [\n",
    "    OnehotAccuracy('accuracy', **loss_kwargs),\n",
    "    DummyAccuracy('dummy-accuracy', **loss_kwargs),\n",
    "]\n",
    "\n",
    "from fuzzytorch import C_\n",
    "trainh_config = {\n",
    "    'early_stop_epochcheck_epochs':1, # every n epochs check\n",
    "    #'early_stop_epochcheck_epochs':2, # every n epochs check\n",
    "    'early_stop_patience_epochchecks':int(1e2),\n",
    "    #'save_mode':C_.SM_NO_SAVE,\n",
    "    #'save_mode':C_.SM_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_INF_METRIC,\n",
    "    #'save_mode':C_.SM_ONLY_INF_LOSS,\n",
    "    'save_mode':C_.SM_ONLY_SUP_METRIC,\n",
    "}\n",
    "model = mdl_params['mdl_class'](**mdl_params['mdl_kwargs'])\n",
    "\n",
    "### OPTIMIZER\n",
    "import torch.optim as optims\n",
    "from fuzzytorch.optimizers import LossOptimizer\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'opt_kwargs':{\n",
    "        'lr':1e-3,\n",
    "    },\n",
    "    'decay_kwargs':{\n",
    "        'lr':0.9,\n",
    "    }\n",
    "}\n",
    "optimizer = LossOptimizer(model, optims.Adam, **optimizer_kwargs)\n",
    "\n",
    "from flamingchoripan.prints import print_bar\n",
    "from fuzzytorch.handlers import ModelTrainHandler\n",
    "from fuzzytorch.monitors import LossMonitor\n",
    "\n",
    "loss_monitors = LossMonitor(loss, optimizer, metrics, **trainh_config)\n",
    "\n",
    "mtrain_config = {\n",
    "    'id':0,\n",
    "    'epochs_max':1e3,\n",
    "    'save_rootdir':'../save',\n",
    "}\n",
    "model_train_handler = ModelTrainHandler(model, loss_monitors, **mtrain_config)\n",
    "model_train_handler.build_gpu(gpu_index=None)\n",
    "print(model_train_handler)\n",
    "model_train_handler.fit_loader(train_loader_mnist, val_loader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_time_util_convergence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['opt_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['loss_df_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['metrics_df_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from flamingchoripan.counters import Counter\n",
    "\n",
    "d = {\n",
    "'val_epoch_counter_duration':1,\n",
    "'earlystop_epoch_duration':5,\n",
    "}\n",
    "c = Counter(d)\n",
    "for _ in range(50):\n",
    "    print(c, c.check('earlystop_epoch_duration'))\n",
    "    c.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import flamingChoripan.tinyFlame.plots as tfplots\n",
    "\n",
    "### training plots\n",
    "fig, ax = tfplots.plot_trainloss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_loss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_metrics(train_handler)\n",
    "#fig, ax = tfplots.plot_optimizer(train_handler, save_dir=mtrain_config['images_save_dir'])\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
