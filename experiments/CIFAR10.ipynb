{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../flaming-choripan') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "## Train-Val Split\n",
    "train_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':True,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "val_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':False,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "train_cifar10 = datasets.CIFAR10(**train_kwargs)\n",
    "val_cifar10 = datasets.CIFAR10(**val_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([50000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([50000]) - y.max: 9\n",
      "x: torch.Size([10000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([10000]) - y.max: 9\n",
      "{'input': {'x': (3, 32, 32)-float32-cpu, 'x2': (32)-float32-cpu}, 'target': {'y': ()-int64-cpu, 'y2': (1)-int64-cpu}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "## Batch Sizes\n",
    "train_batch_size = 256\n",
    "val_batch_size = train_batch_size\n",
    "\n",
    "train_dataset_mnist = MyDataset(train_cifar10.data, train_cifar10.targets, uses_da=True)\n",
    "val_dataset_mnist = MyDataset(val_cifar10.data, val_cifar10.targets)\n",
    "val_dataset_mnist.set_norm_values(*train_dataset_mnist.get_norm_values())\n",
    "\n",
    "print(train_dataset_mnist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input': {'x': (256, 3, 32, 32)-float32-cpu, 'x2': (256, 32)-float32-cpu}, 'target': {'y': (256)-int64-cpu, 'y2': (256, 1)-int64-cpu}}\n",
      "data torch.Size([256, 3, 32, 32]) cpu torch.float32 tensor(-2.1346) tensor(2.3068)\n",
      "target torch.Size([256]) cpu torch.int64 tensor(0) tensor(9)\n",
      "196\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzytorch.datasets import tensor_data_collate\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist, batch_size=train_batch_size, shuffle=True, collate_fn=tensor_data_collate)\n",
    "val_loader_mnist = torch.utils.data.DataLoader(val_dataset_mnist, batch_size=val_batch_size, collate_fn=tensor_data_collate)\n",
    "\n",
    "# print example\n",
    "for k,tensor_dict in enumerate(train_loader_mnist):\n",
    "#for k,(data, target) in enumerate(val_loader_mnist):\n",
    "    print(tensor_dict)\n",
    "    ind = 37\n",
    "    data = tensor_dict['input']['x']\n",
    "    target = tensor_dict['target']['y']\n",
    "    print('data', data.shape, data.device ,data.dtype, data.min(), data.max())\n",
    "    print('target', target.shape, target.device, target.dtype, target.min(), target.max())\n",
    "    img = data[ind].permute(1,2,0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'target: {target[ind]}')\n",
    "    break\n",
    "    \n",
    "print(len(train_loader_mnist))\n",
    "print(len(val_loader_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(0) - {'mdl_class': <class 'baseline_models.CNN2DClassifier'>, 'mdl_kwargs': {'dropout': 0.5, 'cnn_features': [16, 32, 64], 'uses_mlp_classifier': True}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flamingchoripan.datascience.grid_search import GDIter, GridSeacher\n",
    "from baseline_models import MLPClassifier, CNN2DClassifier\n",
    "\n",
    "mdl_params = {\n",
    "    #'mdl_class':MLPClassifier,\n",
    "    'mdl_class':CNN2DClassifier,\n",
    "    'mdl_kwargs':{\n",
    "        'dropout':0.5,\n",
    "        #'dropout':0.0,\n",
    "        'cnn_features':[16, 32, 64],\n",
    "        #'cnn_features':[16, 32],\n",
    "        'uses_mlp_classifier':True,\n",
    "        #'uses_mlp_classifier':False,\n",
    "    },\n",
    "}\n",
    "gs = GridSeacher(mdl_params)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ml_cnn2d: Conv2D(\n",
      "  (0) - Conv2DLinear(input_dims=3, input_space=[32, 32], output_dims=16, output_space=[16, 16], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(1,216[p])\n",
      "  (1) - Conv2DLinear(input_dims=16, input_space=[16, 16], output_dims=32, output_space=[8, 8], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(12,832[p])\n",
      "  (2) - Conv2DLinear(input_dims=32, input_space=[8, 8], output_dims=64, output_space=[4, 4], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(51,264[p])\n",
      ")(65,312[p])\n",
      "mlp_classifier: MLP(\n",
      "  (0) - Linear(input_dims=1024, output_dims=50, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(51,250[p])\n",
      "  (1) - Linear(input_dims=50, output_dims=10, activation=linear, in_dropout=0.5, out_dropout=0.0, bias=True, split_out=1)(510[p])\n",
      ")(51,760[p])\n",
      "▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34mmodel_name: mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64 - id: 0\u001b[0m\n",
      "\u001b[32mdevice: cpu - device_name: cpu\u001b[0m\n",
      "save_rootdir: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64\n",
      "[x-entropy]\n",
      " - opt-parameters: 117,072[p] - device: cpu\n",
      " - save-mode: only_sup_metric(target_metric_crit: accuracy)\n",
      " - early_stop_epochcheck_epochs: 1 - early_stop_patience_epochchecks: 100\n",
      "\u001b[31m> deleting previous epochs [1] in: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64 (id: 0)\u001b[0m\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  0%|          | 1/196000 [00:00,  8.09it/s, id: 0 - epoch: 1/1,000(0/196)[x-entropy] *loss*: 2.31=1.16+.77(loss/2+loss/3)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../fuzzytorch/handler.py:57: UserWarning: there is not CUDA nor GPUs... Using CPU >:(\n",
      "  warnings.warn('there is not CUDA nor GPUs... Using CPU >:(')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 180/196000 [00:21,  8.40it/s, id: 0 - epoch: 1/1,000(179/196)[x-entropy] *loss*: 1.61=.80+.54(loss/2+loss/3)]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### LOSS\n",
    "from fuzzytorch.losses import CrossEntropy\n",
    "\n",
    "loss_kwargs = {\n",
    "    'model_output_is_with_softmax':False,\n",
    "    'target_is_onehot':False,\n",
    "}\n",
    "loss = CrossEntropy('x-entropy', **loss_kwargs)\n",
    "\n",
    "### METRICS\n",
    "from fuzzytorch.metrics import DummyAccuracy, OnehotAccuracy\n",
    "metrics = [\n",
    "    OnehotAccuracy('accuracy', **loss_kwargs),\n",
    "    DummyAccuracy('dummy-accuracy', **loss_kwargs),\n",
    "]\n",
    "\n",
    "from fuzzytorch import C_\n",
    "trainh_config = {\n",
    "    'early_stop_epochcheck_epochs':1, # every n epochs check\n",
    "    #'early_stop_epochcheck_epochs':2, # every n epochs check\n",
    "    'early_stop_patience_epochchecks':int(1e2),\n",
    "    #'save_mode':C_.SM_NO_SAVE,\n",
    "    #'save_mode':C_.SM_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_INF_METRIC,\n",
    "    #'save_mode':C_.SM_ONLY_INF_LOSS,\n",
    "    'save_mode':C_.SM_ONLY_SUP_METRIC,\n",
    "}\n",
    "model = mdl_params['mdl_class'](**mdl_params['mdl_kwargs'])\n",
    "\n",
    "### OPTIMIZER\n",
    "import torch.optim as optims\n",
    "from fuzzytorch.optimizers import NewOptimizer\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'opt_kwargs':{\n",
    "        'lr':1e-3,\n",
    "    },\n",
    "    'decay_kwargs':{\n",
    "        'lr':0.9,\n",
    "    }\n",
    "}\n",
    "optimizer = NewOptimizer(model, optims.Adam, **optimizer_kwargs)\n",
    "\n",
    "from flamingchoripan.prints import print_bar\n",
    "from fuzzytorch.handler import ModelTrainHandler\n",
    "from fuzzytorch.train_handlers import NewTrainHandler\n",
    "\n",
    "train_handlers = NewTrainHandler(optimizer, loss, metrics, **trainh_config)\n",
    "\n",
    "mtrain_config = {\n",
    "    'id':0,\n",
    "    'epochs_max':1e3,\n",
    "    'save_rootdir':'../save',\n",
    "    #'extra_model_name_dict':{'x':9},\n",
    "}\n",
    "model_train_handler = ModelTrainHandler(model, train_handlers, **mtrain_config)\n",
    "model_train_handler.build_gpu(gpu_index=None)\n",
    "print(model_train_handler)\n",
    "model_train_handler.fit_loader(train_loader_mnist, val_loader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import flamingChoripan.tinyFlame.plots as tfplots\n",
    "\n",
    "### training plots\n",
    "fig, ax = tfplots.plot_trainloss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_loss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_metrics(train_handler)\n",
    "#fig, ax = tfplots.plot_optimizer(train_handler, save_dir=mtrain_config['images_save_dir'])\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
