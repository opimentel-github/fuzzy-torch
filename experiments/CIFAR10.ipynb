{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../flaming-choripan') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4dd329c3394d3aaeadb93b2cfb6799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "## Train-Val Split\n",
    "train_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':True,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "val_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':False,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "train_cifar10 = datasets.CIFAR10(**train_kwargs)\n",
    "val_cifar10 = datasets.CIFAR10(**val_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([50000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([50000]) - y.max: 9\n",
      "x: torch.Size([10000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([10000]) - y.max: 9\n",
      "{input: {x: (3, 32, 32)-float32-cpu}, target: {y: ()-int64-cpu}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import MyDataset\n",
    "import numpy as np\n",
    "from fuzzytorch.utils import print_tdict\n",
    "\n",
    "## Batch Sizes\n",
    "train_batch_size = 256\n",
    "val_batch_size = train_batch_size\n",
    "\n",
    "train_dataset_mnist = MyDataset(train_cifar10.data, train_cifar10.targets, uses_da=True)\n",
    "val_dataset_mnist = MyDataset(val_cifar10.data, val_cifar10.targets)\n",
    "val_dataset_mnist.set_norm_values(*train_dataset_mnist.get_norm_values())\n",
    "print_tdict(train_dataset_mnist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{input: {x: (256, 3, 32, 32)-float32-cpu}, target: {y: (256)-int64-cpu}}\n",
      "196\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist, batch_size=train_batch_size, shuffle=True)\n",
    "val_loader_mnist = torch.utils.data.DataLoader(val_dataset_mnist, batch_size=val_batch_size)\n",
    "\n",
    "# print example\n",
    "for k,tensor_dict in enumerate(train_loader_mnist):\n",
    "#for k,(data, target) in enumerate(val_loader_mnist):\n",
    "    print_tdict(tensor_dict)\n",
    "    ind = 37\n",
    "    data = tensor_dict['input']['x']\n",
    "    target = tensor_dict['target']['y']\n",
    "    img = data[ind].permute(1,2,0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'target: {target[ind]}')\n",
    "    break\n",
    "    \n",
    "print(len(train_loader_mnist))\n",
    "print(len(val_loader_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(0) - {'mdl_class': <class 'baseline_models.CNN2DClassifier'>, 'mdl_kwargs': {'dropout': 0.5, 'cnn_features': [16, 32, 64], 'uses_mlp_classifier': True}}\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flamingchoripan.datascience.grid_search import GDIter, GridSeacher\n",
    "from baseline_models import MLPClassifier, CNN2DClassifier\n",
    "\n",
    "mdl_params = {\n",
    "    #'mdl_class':MLPClassifier,\n",
    "    'mdl_class':CNN2DClassifier,\n",
    "    'mdl_kwargs':{\n",
    "        'dropout':0.5,\n",
    "        #'dropout':0.0,\n",
    "        'cnn_features':[16, 32, 64],\n",
    "        #'cnn_features':[16, 32],\n",
    "        'uses_mlp_classifier':True,\n",
    "        #'uses_mlp_classifier':False,\n",
    "    },\n",
    "}\n",
    "gs = GridSeacher(mdl_params)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ml_cnn2d: Conv2D(\n",
      "  (0) - Conv2DLinear(input_dims=3, input_space=[32, 32], output_dims=16, output_space=[16, 16], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(1,216[p])\n",
      "  (1) - Conv2DLinear(input_dims=16, input_space=[16, 16], output_dims=32, output_space=[8, 8], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(12,832[p])\n",
      "  (2) - Conv2DLinear(input_dims=32, input_space=[8, 8], output_dims=64, output_space=[4, 4], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(51,264[p])\n",
      ")(65,312[p])\n",
      "mlp_classifier: MLP(\n",
      "  (0) - Linear(input_dims=1024, output_dims=50, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(51,250[p])\n",
      "  (1) - Linear(input_dims=50, output_dims=10, activation=linear, in_dropout=0.5, out_dropout=0.0, bias=True, split_out=1)(510[p])\n",
      ")(51,760[p])\n",
      "▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34mmodel_name: mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16.32.64(117,072[p])\u001b[0m\n",
      "\u001b[34mid: 0\u001b[0m\n",
      "\u001b[32mdevice: cpu - device_name: cpu\u001b[0m\n",
      "save_rootdir: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16.32.64\n",
      "[xentropy]\n",
      " - opt-parameters: 117,072[p] - device: cpu\n",
      " - save-mode: only_inf_loss\n",
      " - counter_k: k(0/0) - counter_epoch: val_epoch(0/0)»earlystop_epoch(0/21)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  0%|          | 5/196000 [03:42, 44.52s/it, id: 0 - epoch: 5/1,000(195/196)[xentropy] b: 80 - __loss__: 1.11 (loss*2=2.22|loss*3=3.32) #.028[segs]\u001b[34m[train][xentropy] __loss__: 1.09 (loss*2=2.17|loss*3=3.26) - accuracy: 62.10 - b-accuracy: 62.11 - dummy-accuracy: 10.00 #12.965[segs]\u001b[0m\u001b[31m[val][xentropy] __loss__: 1.11 (loss*2=2.22|loss*3=3.33) - accuracy: 61.39 - b-accuracy: 61.31 - dummy-accuracy: 10.00 #1.985[segs]\u001b[0m\u001b[33m[stop][xentropy] counter_epoch: val_epoch(0/0)»earlystop_epoch(1/21)\u001b[0m] \n",
      "\u001b[31m*** ctrl+c ***\u001b[0m\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "End of training!!!\n",
      "[xentropy] best_epoch: 4 - time_per_iteration: .07±.01[segs] - time_per_epoch: 7.99±6.29[mins] - total_time: 2.680498[mins]\n",
      "▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID' # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '' # CPU\n",
    "\n",
    "### LOSS\n",
    "from fuzzytorch.losses import XEntropy\n",
    "\n",
    "loss_kwargs = {\n",
    "    'model_output_is_with_softmax':False,\n",
    "    'target_is_onehot':False,\n",
    "}\n",
    "loss = XEntropy('xentropy', **loss_kwargs)\n",
    "\n",
    "### METRICS\n",
    "from fuzzytorch.metrics import DummyAccuracy, Accuracy\n",
    "metrics = [\n",
    "    Accuracy('accuracy', balanced=False, **loss_kwargs),\n",
    "    Accuracy('b-accuracy', balanced=True, **loss_kwargs),\n",
    "    DummyAccuracy('dummy-accuracy', **loss_kwargs),\n",
    "]\n",
    "\n",
    "### GET MODEL\n",
    "model = mdl_params['mdl_class'](**mdl_params['mdl_kwargs'])\n",
    "\n",
    "### OPTIMIZER\n",
    "import torch.optim as optims\n",
    "from fuzzytorch.optimizers import LossOptimizer\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'opt_kwargs':{\n",
    "        'lr':1e-3,\n",
    "    },\n",
    "    'decay_kwargs':{\n",
    "        'lr':0.9,\n",
    "    }\n",
    "}\n",
    "optimizer = LossOptimizer(model, optims.Adam, **optimizer_kwargs)\n",
    "\n",
    "### MONITORS\n",
    "from flamingchoripan.prints import print_bar\n",
    "from fuzzytorch.handlers import ModelTrainHandler\n",
    "from fuzzytorch.monitors import LossMonitor\n",
    "from fuzzytorch import C_\n",
    "\n",
    "monitor_config = {\n",
    "    'val_epoch_counter_duration':0, # every k epochs check\n",
    "    #'val_epoch_counter_duration':2, # every k epochs check\n",
    "    #'earlystop_epoch_duration':1e2,\n",
    "    #'save_mode':C_.SM_NO_SAVE,\n",
    "    #'save_mode':C_.SM_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_INF_METRIC,\n",
    "    'save_mode':C_.SM_ONLY_INF_LOSS,\n",
    "    #'save_mode':C_.SM_ONLY_SUP_METRIC,\n",
    "}\n",
    "loss_monitors = LossMonitor(loss, optimizer, metrics, **monitor_config)\n",
    "\n",
    "### TRAIN\n",
    "mtrain_config = {\n",
    "    'id':0,\n",
    "    'epochs_max':1e3,\n",
    "    'save_rootdir':'../save',\n",
    "}\n",
    "model_train_handler = ModelTrainHandler(model, loss_monitors, **mtrain_config)\n",
    "model_train_handler.build_gpu(gpu_index=None)\n",
    "print(model_train_handler)\n",
    "model_train_handler.fit_loader(train_loader_mnist, val_loader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_time_util_convergence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['opt_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['loss_df_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['metrics_df_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from flamingchoripan.counters import Counter\n",
    "\n",
    "d = {\n",
    "'val_epoch_counter_duration':1,\n",
    "'earlystop_epoch_duration':5,\n",
    "}\n",
    "c = Counter(d)\n",
    "for _ in range(50):\n",
    "    print(c, c.check('earlystop_epoch_duration'))\n",
    "    c.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import flamingChoripan.tinyFlame.plots as tfplots\n",
    "\n",
    "### training plots\n",
    "fig, ax = tfplots.plot_trainloss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_loss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_metrics(train_handler)\n",
    "#fig, ax = tfplots.plot_optimizer(train_handler, save_dir=mtrain_config['images_save_dir'])\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
