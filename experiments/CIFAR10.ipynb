{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../flaming-choripan') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "## Train-Val Split\n",
    "train_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':True,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "val_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':False,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "train_cifar10 = datasets.CIFAR10(**train_kwargs)\n",
    "val_cifar10 = datasets.CIFAR10(**val_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([50000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([50000]) - y.max: 9\n",
      "x: torch.Size([10000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([10000]) - y.max: 9\n",
      "{'input': {'x': (3, 32, 32)-float32-cpu, 'x2': (32)-float32-cpu}, 'target': {'y': ()-int64-cpu, 'y2': (1)-int64-cpu}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "## Batch Sizes\n",
    "train_batch_size = 256\n",
    "val_batch_size = train_batch_size\n",
    "\n",
    "train_dataset_mnist = MyDataset(train_cifar10.data, train_cifar10.targets, uses_da=True)\n",
    "val_dataset_mnist = MyDataset(val_cifar10.data, val_cifar10.targets)\n",
    "val_dataset_mnist.set_norm_values(*train_dataset_mnist.get_norm_values())\n",
    "\n",
    "print(train_dataset_mnist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input': {'x': (256, 3, 32, 32)-float32-cpu, 'x2': (256, 32)-float32-cpu}, 'target': {'y': (256)-int64-cpu, 'y2': (256, 1)-int64-cpu}}\n",
      "data torch.Size([256, 3, 32, 32]) cpu torch.float32 tensor(-2.1262) tensor(2.2997)\n",
      "target torch.Size([256]) cpu torch.int64 tensor(0) tensor(9)\n",
      "196\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzytorch.datasets import tensor_data_collate\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist, batch_size=train_batch_size, shuffle=True, collate_fn=tensor_data_collate)\n",
    "val_loader_mnist = torch.utils.data.DataLoader(val_dataset_mnist, batch_size=val_batch_size, collate_fn=tensor_data_collate)\n",
    "\n",
    "# print example\n",
    "for k,tensor_dict in enumerate(train_loader_mnist):\n",
    "#for k,(data, target) in enumerate(val_loader_mnist):\n",
    "    print(tensor_dict)\n",
    "    ind = 37\n",
    "    data = tensor_dict['input']['x']\n",
    "    target = tensor_dict['target']['y']\n",
    "    print('data', data.shape, data.device ,data.dtype, data.min(), data.max())\n",
    "    print('target', target.shape, target.device, target.dtype, target.min(), target.max())\n",
    "    img = data[ind].permute(1,2,0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'target: {target[ind]}')\n",
    "    break\n",
    "    \n",
    "print(len(train_loader_mnist))\n",
    "print(len(val_loader_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(0) - {'mdl_class': <class 'baseline_models.CNN2DClassifier'>, 'mdl_kwargs': {'dropout': 0.5, 'cnn_features': [16, 32, 64], 'uses_mlp_classifier': True}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flamingchoripan.datascience.grid_search import GDIter, GridSeacher\n",
    "from baseline_models import MLPClassifier, CNN2DClassifier\n",
    "\n",
    "mdl_params = {\n",
    "    #'mdl_class':MLPClassifier,\n",
    "    'mdl_class':CNN2DClassifier,\n",
    "    'mdl_kwargs':{\n",
    "        'dropout':0.5,\n",
    "        #'dropout':0.0,\n",
    "        'cnn_features':[16, 32, 64],\n",
    "        #'cnn_features':[16, 32],\n",
    "        'uses_mlp_classifier':True,\n",
    "        #'uses_mlp_classifier':False,\n",
    "    },\n",
    "}\n",
    "gs = GridSeacher(mdl_params)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Conv2DLinear(input_dims=5, input_space=[11, 11], output_dims=10, output_space=[4, 4], kernel_size=[3, 3], stride=[1, 1], padding_mode=None, padding=[0, 0], pool_kernel_size=[2, 2], activation=linear, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(460[p])\n",
      "10\n",
      "[4, 4]\n",
      "torch.Size([100, 5, 11, 11])\n",
      "torch.Size([100, 10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fuzzytorch.models.cnn.basics import Conv2DLinear\n",
    "\n",
    "cnn = Conv2DLinear(5, [11,11], 10, kernel_size=3, pool_kernel_size=2)\n",
    "print(cnn)\n",
    "print(cnn.get_output_dims())\n",
    "print(cnn.get_output_space())\n",
    "\n",
    "x = torch.zeros((100,5,11,11))\n",
    "print(x.shape)\n",
    "x = cnn(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ml_cnn2d: MLConv2D(\n",
      "(0) - Conv2DLinear(input_dims=3, input_space=[32, 32], output_dims=16, output_space=[16, 16], kernel_size=[5, 5], stride=[1, 1], padding_mode=same, padding=[2, 2], pool_kernel_size=[2, 2], activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(1,216[p])\n",
      "(1) - Conv2DLinear(input_dims=16, input_space=[16, 16], output_dims=32, output_space=[8, 8], kernel_size=[5, 5], stride=[1, 1], padding_mode=same, padding=[2, 2], pool_kernel_size=[2, 2], activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(12,832[p])\n",
      "(2) - Conv2DLinear(input_dims=32, input_space=[8, 8], output_dims=64, output_space=[4, 4], kernel_size=[5, 5], stride=[1, 1], padding_mode=same, padding=[2, 2], pool_kernel_size=[2, 2], activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(51,264[p])\n",
      ")(65,312[p])\n",
      "mlp_classifier: MLP(\n",
      "(0) - Linear(input_dims=1024, output_dims=50, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(51,250[p])\n",
      "(1) - Linear(input_dims=50, output_dims=10, activation=linear, in_dropout=0.5, out_dropout=0.0, bias=True, split_out=1)(510[p])\n",
      ")(51,760[p])\n",
      "▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34mmodel_name: mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64 - id: 0\u001b[0m\n",
      "\u001b[32mdevice: cpu - device_name: cpu\u001b[0m\n",
      "save_rootdir: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64\n",
      "[x-entropy]\n",
      " - opt-parameters: 117,072[p] - device: cpu\n",
      " - save-mode: only_sup_metric(target_metric_crit: accuracy)\n",
      " - early_stop_epochcheck_epochs: 1 - early_stop_patience_epochchecks: 100\n",
      "\u001b[33m> creating dir: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64\u001b[0m\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  0%|          | 1/196000 [00:00,  8.89it/s, id: 0 - epoch: 1/1,000(0/196)[x-entropy] *loss*: 2.30=1.15+.77(loss/2+loss/3)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../fuzzytorch/handler.py:57: UserWarning: there is not CUDA nor GPUs... Using CPU >:(\n",
      "  warnings.warn('there is not CUDA nor GPUs... Using CPU >:(')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 929/196000 [02:24, 10.13it/s, id: 0 - epoch: 5/1,000(145/196)[x-entropy] *loss*: 1.01=.51+.34(loss/2+loss/3)\u001b[34m[train][x-entropy] *loss*: 1.21=.61+.40(loss/2+loss/3) - accuracy: 61.48 - dummy-accuracy: 10.00 (time: 0.1878[mins])\u001b[0m\u001b[31m[val][x-entropy] *loss*: 1.07=.54+.36(loss/2+loss/3) - accuracy: 60.37 - dummy-accuracy: 10.00 (time: 0.0259[mins])\u001b[0m\u001b[33m[stop][x-entropy] epoch-counter: (0/1) - patience: (1/100)\u001b[0m]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### LOSS\n",
    "from fuzzytorch.losses import CrossEntropy\n",
    "\n",
    "loss_kwargs = {\n",
    "    'model_output_is_with_softmax':False,\n",
    "    'target_is_onehot':False,\n",
    "}\n",
    "loss = CrossEntropy('x-entropy', **loss_kwargs)\n",
    "\n",
    "### METRICS\n",
    "from fuzzytorch.metrics import DummyAccuracy, OnehotAccuracy\n",
    "metrics = [\n",
    "    OnehotAccuracy('accuracy', **loss_kwargs),\n",
    "    DummyAccuracy('dummy-accuracy', **loss_kwargs),\n",
    "]\n",
    "\n",
    "from fuzzytorch import C_\n",
    "trainh_config = {\n",
    "    'early_stop_epochcheck_epochs':1, # every n epochs check\n",
    "    #'early_stop_epochcheck_epochs':2, # every n epochs check\n",
    "    'early_stop_patience_epochchecks':int(1e2),\n",
    "    #'save_mode':C_.SM_NO_SAVE,\n",
    "    #'save_mode':C_.SM_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_INF_METRIC,\n",
    "    #'save_mode':C_.SM_ONLY_INF_LOSS,\n",
    "    'save_mode':C_.SM_ONLY_SUP_METRIC,\n",
    "}\n",
    "model = mdl_params['mdl_class'](**mdl_params['mdl_kwargs'])\n",
    "\n",
    "### OPTIMIZER\n",
    "import torch.optim as optims\n",
    "from fuzzytorch.optimizers import NewOptimizer\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'opt_kwargs':{\n",
    "        'lr':1e-3,\n",
    "    },\n",
    "    'decay_kwargs':{\n",
    "        'lr':0.9,\n",
    "    }\n",
    "}\n",
    "optimizer = NewOptimizer(model, optims.Adam, **optimizer_kwargs)\n",
    "\n",
    "from flamingchoripan.prints import print_bar\n",
    "from fuzzytorch.handler import ModelTrainHandler\n",
    "from fuzzytorch.train_handlers import NewTrainHandler\n",
    "\n",
    "train_handlers = NewTrainHandler(optimizer, loss, metrics, **trainh_config)\n",
    "\n",
    "mtrain_config = {\n",
    "    'id':0,\n",
    "    'epochs_max':1e3,\n",
    "    'save_rootdir':'../save',\n",
    "    #'extra_model_name_dict':{'x':9},\n",
    "}\n",
    "model_train_handler = ModelTrainHandler(model, train_handlers, **mtrain_config)\n",
    "model_train_handler.build_gpu(gpu_index=None)\n",
    "print(model_train_handler)\n",
    "model_train_handler.fit_loader(train_loader_mnist, val_loader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import flamingChoripan.tinyFlame.plots as tfplots\n",
    "\n",
    "### training plots\n",
    "fig, ax = tfplots.plot_trainloss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_loss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_metrics(train_handler)\n",
    "#fig, ax = tfplots.plot_optimizer(train_handler, save_dir=mtrain_config['images_save_dir'])\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
