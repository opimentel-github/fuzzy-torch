{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../flaming-choripan') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "## Train-Val Split\n",
    "train_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':True,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "val_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':False,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "train_cifar10 = datasets.CIFAR10(**train_kwargs)\n",
    "val_cifar10 = datasets.CIFAR10(**val_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([50000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([50000]) - y.max: 9\n",
      "x: torch.Size([10000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([10000]) - y.max: 9\n",
      "{'input': {'x': (3, 32, 32)-float32-cpu, 'x2': (32)-float32-cpu}, 'target': {'y': ()-int64-cpu, 'y2': (1)-int64-cpu}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "## Batch Sizes\n",
    "train_batch_size = 256\n",
    "val_batch_size = train_batch_size\n",
    "\n",
    "train_dataset_mnist = MyDataset(train_cifar10.data, train_cifar10.targets, uses_da=True)\n",
    "val_dataset_mnist = MyDataset(val_cifar10.data, val_cifar10.targets)\n",
    "val_dataset_mnist.set_norm_values(*train_dataset_mnist.get_norm_values())\n",
    "\n",
    "print(train_dataset_mnist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input': {'x': (256, 3, 32, 32)-float32-cpu, 'x2': (256, 32)-float32-cpu}, 'target': {'y': (256)-int64-cpu, 'y2': (256, 1)-int64-cpu}}\n",
      "data torch.Size([256, 3, 32, 32]) cpu torch.float32 tensor(-2.1637) tensor(2.3005)\n",
      "target torch.Size([256]) cpu torch.int64 tensor(0) tensor(9)\n",
      "196\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzytorch.datasets import tensor_data_collate\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist, batch_size=train_batch_size, shuffle=True, collate_fn=tensor_data_collate)\n",
    "val_loader_mnist = torch.utils.data.DataLoader(val_dataset_mnist, batch_size=val_batch_size, collate_fn=tensor_data_collate)\n",
    "\n",
    "# print example\n",
    "for k,tensor_dict in enumerate(train_loader_mnist):\n",
    "#for k,(data, target) in enumerate(val_loader_mnist):\n",
    "    print(tensor_dict)\n",
    "    ind = 37\n",
    "    data = tensor_dict['input']['x']\n",
    "    target = tensor_dict['target']['y']\n",
    "    print('data', data.shape, data.device ,data.dtype, data.min(), data.max())\n",
    "    print('target', target.shape, target.device, target.dtype, target.min(), target.max())\n",
    "    img = data[ind].permute(1,2,0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'target: {target[ind]}')\n",
    "    break\n",
    "    \n",
    "print(len(train_loader_mnist))\n",
    "print(len(val_loader_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(0) - {'mdl_class': <class 'baseline_models.CNN2DClassifier'>, 'mdl_kwargs': {'dropout': 0.5, 'cnn_features': [16, 32, 64], 'uses_mlp_classifier': True}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flamingchoripan.datascience.grid_search import GDIter, GridSeacher\n",
    "from baseline_models import MLPClassifier, CNN2DClassifier\n",
    "\n",
    "mdl_params = {\n",
    "    #'mdl_class':MLPClassifier,\n",
    "    'mdl_class':CNN2DClassifier,\n",
    "    'mdl_kwargs':{\n",
    "        'dropout':0.5,\n",
    "        #'dropout':0.0,\n",
    "        'cnn_features':[16, 32, 64],\n",
    "        #'cnn_features':[16, 32],\n",
    "        'uses_mlp_classifier':True,\n",
    "        #'uses_mlp_classifier':False,\n",
    "    },\n",
    "}\n",
    "gs = GridSeacher(mdl_params)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "ml_cnn2d: Conv2D(\n",
      "  (0) - Conv2DLinear(input_dims=3, input_space=[32, 32], output_dims=16, output_space=[16, 16], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(1,216[p])\n",
      "  (1) - Conv2DLinear(input_dims=16, input_space=[16, 16], output_dims=32, output_space=[8, 8], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(12,832[p])\n",
      "  (2) - Conv2DLinear(input_dims=32, input_space=[8, 8], output_dims=64, output_space=[4, 4], spatial_field=[6, 6], cnn_kwargs={'kernel_size': [5, 5], 'stride': [1, 1], 'dilation': [1, 1]}, pool_kwargs={'kernel_size': [2, 2], 'stride': [2, 2], 'dilation': [1, 1]}, padding_mode=same, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True)(51,264[p])\n",
      ")(65,312[p])\n",
      "mlp_classifier: MLP(\n",
      "  (0) - Linear(input_dims=1024, output_dims=50, activation=relu, in_dropout=0.0, out_dropout=0.0, bias=True, split_out=1)(51,250[p])\n",
      "  (1) - Linear(input_dims=50, output_dims=10, activation=linear, in_dropout=0.5, out_dropout=0.0, bias=True, split_out=1)(510[p])\n",
      ")(51,760[p])\n",
      "▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[34mmodel_name: mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64 - id: 0\u001b[0m\n",
      "\u001b[32mdevice: cpu - device_name: cpu\u001b[0m\n",
      "save_rootdir: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64\n",
      "[x-entropy]\n",
      " - opt-parameters: 117,072[p] - device: cpu\n",
      " - save-mode: only_sup_metric(target_metric_crit: accuracy)\n",
      " - counter_k: k: 0/0 - counter_epoch: val_epoch: 0/0 > earlystop_epoch: 0/21\n",
      "\u001b[31m> (id: 0) deleting previous epochs: [77] in: ../save/mdl=cnn2d°dropout=0.5°output_dims=10°cnn_features=16-32-64\u001b[0m\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "  0%|          | 0/196000 [00:00, ?it/s, id: 0 - epoch: 0/1,000(0/196)[x-entropy] __loss__: 2.31=1.15+.77(loss/2+loss/3) @.090[segs]]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../fuzzytorch/handlers.py:57: UserWarning: there is not CUDA nor GPUs... Using CPU >:(\n",
      "  warnings.warn('there is not CUDA nor GPUs... Using CPU >:(')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/196000 [00:06, ?it/s, id: 0 - epoch: 0/1,000(54/196)[x-entropy] __loss__: 2.02=1.01+.67(loss/2+loss/3) @.088[segs]]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### LOSS\n",
    "from fuzzytorch.losses import CrossEntropy\n",
    "\n",
    "loss_kwargs = {\n",
    "    'model_output_is_with_softmax':False,\n",
    "    'target_is_onehot':False,\n",
    "}\n",
    "loss = CrossEntropy('x-entropy', **loss_kwargs)\n",
    "\n",
    "### METRICS\n",
    "from fuzzytorch.metrics import DummyAccuracy, OnehotAccuracy\n",
    "metrics = [\n",
    "    OnehotAccuracy('accuracy', **loss_kwargs),\n",
    "    DummyAccuracy('dummy-accuracy', **loss_kwargs),\n",
    "]\n",
    "\n",
    "from fuzzytorch import C_\n",
    "trainh_config = {\n",
    "    'early_stop_epochcheck_epochs':1, # every n epochs check\n",
    "    #'early_stop_epochcheck_epochs':2, # every n epochs check\n",
    "    'early_stop_patience_epochchecks':int(1e2),\n",
    "    #'save_mode':C_.SM_NO_SAVE,\n",
    "    #'save_mode':C_.SM_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_INF_METRIC,\n",
    "    #'save_mode':C_.SM_ONLY_INF_LOSS,\n",
    "    'save_mode':C_.SM_ONLY_SUP_METRIC,\n",
    "}\n",
    "model = mdl_params['mdl_class'](**mdl_params['mdl_kwargs'])\n",
    "\n",
    "### OPTIMIZER\n",
    "import torch.optim as optims\n",
    "from fuzzytorch.optimizers import LossOptimizer\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'opt_kwargs':{\n",
    "        'lr':1e-3,\n",
    "    },\n",
    "    'decay_kwargs':{\n",
    "        'lr':0.9,\n",
    "    }\n",
    "}\n",
    "optimizer = LossOptimizer(model, optims.Adam, **optimizer_kwargs)\n",
    "\n",
    "from flamingchoripan.prints import print_bar\n",
    "from fuzzytorch.handlers import ModelTrainHandler\n",
    "from fuzzytorch.monitors import LossMonitor\n",
    "\n",
    "loss_monitors = LossMonitor(loss, optimizer, metrics, **trainh_config)\n",
    "\n",
    "mtrain_config = {\n",
    "    'id':0,\n",
    "    'epochs_max':1e3,\n",
    "    'save_rootdir':'../save',\n",
    "}\n",
    "model_train_handler = ModelTrainHandler(model, loss_monitors, **mtrain_config)\n",
    "model_train_handler.build_gpu(gpu_index=None)\n",
    "print(model_train_handler)\n",
    "model_train_handler.fit_loader(train_loader_mnist, val_loader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_time_util_convergence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['opt_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['loss_df_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df opt_df loss_df_epoch metrics_df_epoch\n",
    "loss_monitors.get_save_dict()['metrics_df_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from flamingchoripan.counters import Counter\n",
    "\n",
    "d = {\n",
    "'val_epoch_counter_duration':1,\n",
    "'earlystop_epoch_duration':5,\n",
    "}\n",
    "c = Counter(d)\n",
    "for _ in range(50):\n",
    "    print(c, c.check('earlystop_epoch_duration'))\n",
    "    c.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import flamingChoripan.tinyFlame.plots as tfplots\n",
    "\n",
    "### training plots\n",
    "fig, ax = tfplots.plot_trainloss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_loss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_metrics(train_handler)\n",
    "#fig, ax = tfplots.plot_optimizer(train_handler, save_dir=mtrain_config['images_save_dir'])\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
