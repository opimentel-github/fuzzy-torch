{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../flaming-choripan') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import numpy as np\n",
    "\n",
    "## Train-Val Split\n",
    "train_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':True,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "val_kwargs = {\n",
    "    'root':'../data/',\n",
    "    'train':False,\n",
    "    'download':True,\n",
    "    'transform':transforms.Compose([transforms.ToTensor()]),\n",
    "}\n",
    "train_cifar10 = datasets.CIFAR10(**train_kwargs)\n",
    "val_cifar10 = datasets.CIFAR10(**val_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([50000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([50000]) - y.max: 9\n",
      "x: torch.Size([10000, 3, 32, 32]) - x.max: 1.0 - y: torch.Size([10000]) - y.max: 9\n",
      "{'input': {'x': (3, 32, 32)-float32-cpu, 'x2': (32)-float32-cpu}, 'target': {'y': ()-int64-cpu, 'y2': (1)-int64-cpu}}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import MyDataset\n",
    "import numpy as np\n",
    "\n",
    "## Batch Sizes\n",
    "train_batch_size = 256\n",
    "val_batch_size = train_batch_size\n",
    "\n",
    "train_dataset_mnist = MyDataset(train_cifar10.data, train_cifar10.targets, uses_da=True)\n",
    "val_dataset_mnist = MyDataset(val_cifar10.data, val_cifar10.targets)\n",
    "val_dataset_mnist.set_norm_values(*train_dataset_mnist.get_norm_values())\n",
    "\n",
    "print(train_dataset_mnist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input': {'x': (256, 3, 32, 32)-float32-cpu, 'x2': (256, 32)-float32-cpu}, 'target': {'y': (256)-int64-cpu, 'y2': (256, 1)-int64-cpu}}\n",
      "data torch.Size([256, 3, 32, 32]) cpu torch.float32 tensor(-2.1507) tensor(2.3020)\n",
      "target torch.Size([256]) cpu torch.int64 tensor(0) tensor(9)\n",
      "196\n",
      "40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzytorch.datasets import tensor_data_collate\n",
    "\n",
    "## DataLoaders\n",
    "train_loader_mnist = torch.utils.data.DataLoader(train_dataset_mnist, batch_size=train_batch_size, shuffle=True, collate_fn=tensor_data_collate)\n",
    "val_loader_mnist = torch.utils.data.DataLoader(val_dataset_mnist, batch_size=val_batch_size, collate_fn=tensor_data_collate)\n",
    "\n",
    "# print example\n",
    "for k,tensor_dict in enumerate(train_loader_mnist):\n",
    "#for k,(data, target) in enumerate(val_loader_mnist):\n",
    "    print(tensor_dict)\n",
    "    ind = 37\n",
    "    data = tensor_dict['input']['x']\n",
    "    target = tensor_dict['target']['y']\n",
    "    print('data', data.shape, data.device ,data.dtype, data.min(), data.max())\n",
    "    print('target', target.shape, target.device, target.dtype, target.min(), target.max())\n",
    "    img = data[ind].permute(1,2,0).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'target: {target[ind]}')\n",
    "    break\n",
    "    \n",
    "print(len(train_loader_mnist))\n",
    "print(len(val_loader_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(0) - {'mdl_class': <class 'baseline_models.CNN2DClassifier'>, 'mdl_kwargs': {'dropout': 0.5, 'cnn_features': [16, 32, 64], 'uses_mlp_classifier': True}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from flamingchoripan.datascience.grid_search import GDIter, GridSeacher\n",
    "from baseline_models import MLPClassifier, CNN2DClassifier\n",
    "\n",
    "mdl_params = {\n",
    "    #'mdl_class':MLPClassifier,\n",
    "    'mdl_class':CNN2DClassifier,\n",
    "    'mdl_kwargs':{\n",
    "        'dropout':0.5,\n",
    "        #'dropout':0.0,\n",
    "        'cnn_features':[16, 32, 64],\n",
    "        #'cnn_features':[16, 32],\n",
    "        'uses_mlp_classifier':True,\n",
    "        #'uses_mlp_classifier':False,\n",
    "    },\n",
    "}\n",
    "gs = GridSeacher(mdl_params)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Conv2DLinear(input_dims=5, output_dims=10, kernel_size=[3, 3], input_space=[11, 11], output_space=[4, 4], in_dropout=0.0, activation=linear, bias=True, padding=[0, 0], stride=[1, 1], pool_kernel_size=[2, 2], out_drop=0.0, out_drop=0.0, split_out=1)(460[p])\n",
      "10\n",
      "[4, 4]\n",
      "torch.Size([100, 5, 11, 11])\n",
      "torch.Size([100, 10, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fuzzytorch.models.cnn.basics import Conv2DLinear\n",
    "\n",
    "cnn = Conv2DLinear(5, [11,11], 10, kernel_size=3, pool_kernel_size=2)\n",
    "print(cnn)\n",
    "print(cnn.get_output_dims())\n",
    "print(cnn.get_output_space())\n",
    "\n",
    "x = torch.zeros((100,5,11,11))\n",
    "print(x.shape)\n",
    "x = cnn(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "MLP(\n",
      "(0) - Linear(input_dims=100, output_dims=20, activation=relu, bias=True, in_dropout=0.0, out_drop=0.0, split_out=1)(2,020[p])\n",
      "(1) - Linear(input_dims=20, output_dims=5, activation=relu, bias=True, in_dropout=0.0, out_drop=0.0, split_out=1)(105[p])\n",
      "(2) - Linear(input_dims=5, output_dims=10, activation=relu, bias=True, in_dropout=0.0, out_drop=0.0, split_out=1)(60[p])\n",
      ")(2,185[p])\n",
      "Conv2DLinear(input_dims=1000, output_dims=200, kernel_size=[3, 3], input_space=[11, 11], output_space=[9, 9], in_dropout=0.0, activation=linear, bias=True, padding=[0, 0], stride=[1, 1], pool_kernel_size=[1, 1], out_drop=0.0, out_drop=0.0, split_out=1)(1,800,200[p])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f3d3760bfcc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0membd_dims_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membd_dims_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oscar/tesis/fuzzy-torch/fuzzytorch/models/cnn/basics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dims, input_space, output_dims, embd_dims_list, activation, in_dropout, out_dropout, bias, uses_custom_non_linear_init, split_out, stride, kernel_size, padding_mode, padding, pool_kernel_size, dropout, last_activation, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m                         \u001b[0minput_space_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                         \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fuzzytorch.models.basics import MLP\n",
    "from fuzzytorch.models.cnn.basics import MLConv2D\n",
    "\n",
    "print(MLP(100,10,[20,5]))\n",
    "\n",
    "embd_dims_list = [200, 100]\n",
    "cnn = MLConv2D(1000, [11,11], 10, embd_dims_list)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleAttributeError",
     "evalue": "'CNN2DClassifier' object has no attribute 'input2d_dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fbd51ef28399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m'save_mode'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mC_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSM_ONLY_SUP_METRIC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m }\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mdl_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmdl_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mdl_kwargs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m### OPTIMIZER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/oscar/tesis/fuzzy-torch/experiments/baseline_models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dropout, cnn_features, uses_mlp_classifier, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn2d_embedding_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mcnn_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0minput2d_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput2d_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcnn_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mcnn2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput2d_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcnn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lchandler/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'CNN2DClassifier' object has no attribute 'input2d_dims'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### LOSS\n",
    "from fuzzytorch.losses import CrossEntropy\n",
    "\n",
    "loss_kwargs = {\n",
    "    'model_output_is_with_softmax':False,\n",
    "    'target_is_onehot':False,\n",
    "}\n",
    "loss = CrossEntropy('x-entropy', **loss_kwargs)\n",
    "\n",
    "### METRICS\n",
    "from fuzzytorch.metrics import DummyAccuracy, OnehotAccuracy\n",
    "metrics = [\n",
    "    OnehotAccuracy('accuracy', **loss_kwargs),\n",
    "    DummyAccuracy('dummy-accuracy', **loss_kwargs),\n",
    "]\n",
    "\n",
    "from fuzzytorch import C_\n",
    "trainh_config = {\n",
    "    'early_stop_epochcheck_epochs':1, # every n epochs check\n",
    "    #'early_stop_epochcheck_epochs':2, # every n epochs check\n",
    "    'early_stop_patience_epochchecks':int(1e2),\n",
    "    #'save_mode':C_.SM_NO_SAVE,\n",
    "    #'save_mode':C_.SM_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_ALL,\n",
    "    #'save_mode':C_.SM_ONLY_INF_METRIC,\n",
    "    #'save_mode':C_.SM_ONLY_INF_LOSS,\n",
    "    'save_mode':C_.SM_ONLY_SUP_METRIC,\n",
    "}\n",
    "model = mdl_params['mdl_class'](**mdl_params['mdl_kwargs'])\n",
    "\n",
    "### OPTIMIZER\n",
    "import torch.optim as optims\n",
    "from fuzzytorch.optimizers import NewOptimizer\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'opt_kwargs':{\n",
    "        'lr':1e-3,\n",
    "    },\n",
    "    'decay_kwargs':{\n",
    "        'lr':0.9,\n",
    "    }\n",
    "}\n",
    "optimizer = NewOptimizer(model, optims.Adam, **optimizer_kwargs)\n",
    "\n",
    "from flamingchoripan.prints import print_bar\n",
    "from fuzzytorch.handler import ModelTrainHandler\n",
    "from fuzzytorch.train_handlers import NewTrainHandler\n",
    "\n",
    "train_handlers = NewTrainHandler(optimizer, loss, metrics, **trainh_config)\n",
    "\n",
    "mtrain_config = {\n",
    "    'id':0,\n",
    "    'epochs_max':1e3,\n",
    "    'save_rootdir':'../save',\n",
    "    #'extra_model_name_dict':{'x':9},\n",
    "}\n",
    "model_train_handler = ModelTrainHandler(model, train_handlers, **mtrain_config)\n",
    "model_train_handler.build_gpu(gpu_index=None)\n",
    "print(model_train_handler)\n",
    "model_train_handler.fit_loader(train_loader_mnist, val_loader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import flamingChoripan.tinyFlame.plots as tfplots\n",
    "\n",
    "### training plots\n",
    "fig, ax = tfplots.plot_trainloss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_loss(train_handler)\n",
    "fig, ax = tfplots.plot_evaluation_metrics(train_handler)\n",
    "#fig, ax = tfplots.plot_optimizer(train_handler, save_dir=mtrain_config['images_save_dir'])\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and CM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
